{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parkinsons-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start=time()\n",
    "        res=fn(*args, **kwargs)\n",
    "        print(fn.__name__, \"took\", time()-start, \"seconds.\")\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manoj\\OneDrive\\Desktop\\ml\\data\\control\n"
     ]
    }
   ],
   "source": [
    "print(os.path.abspath(control_data_path))  # Shows the full path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_data_path=os.path.join('data', 'control')\n",
    "parkinson_data_path=os.path.join('data', 'parkinson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data\\\\control'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m control_file_list\u001b[38;5;241m=\u001b[39m[os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(control_data_path, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_data_path\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m      2\u001b[0m parkinson_file_list\u001b[38;5;241m=\u001b[39m[os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parkinson_data_path, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(parkinson_data_path)]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data\\\\control'"
     ]
    }
   ],
   "source": [
    "control_file_list=[os.path.join(control_data_path, x) for x in os.listdir(control_data_path)]\n",
    "parkinson_file_list=[os.path.join(parkinson_data_path, x) for x in os.listdir(parkinson_data_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "1. No of strokes\n",
    "2. Stroke speed\n",
    "3. Velocity\n",
    "4. Acceleration\n",
    "5. Jerk\n",
    "6. Horizontal velocity/acceleration/jerk\n",
    "7. Vertical velocity/acceleration/jerk\n",
    "8. Number of changes in velocity direction\n",
    "9. Number of changes in acceleration direction\n",
    "10. Relative NCV\n",
    "11. Relative NCA\n",
    "12. in-air time\n",
    "13. on-surface time\n",
    "14. normalized in-air time\n",
    "15. normalized on-surface time\n",
    "16. in-air/on-surface ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_row=[\"X\", \"Y\", \"Z\", \"Pressure\" , \"GripAngle\" , \"Timestamp\" , \"Test_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@timeit\n",
    "def get_no_strokes(df):\n",
    "    pressure_data=df['Pressure'].as_matrix()\n",
    "    on_surface = (pressure_data>600).astype(int)\n",
    "    return ((np.roll(on_surface, 1) - on_surface) != 0).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@timeit\n",
    "def get_speed(df):\n",
    "    total_dist=0\n",
    "    duration=df['Timestamp'].as_matrix()[-1]\n",
    "    coords=df[['X', 'Y', 'Z']].as_matrix()\n",
    "    for i in range(10, df.shape[0]):\n",
    "        temp=np.linalg.norm(coords[i, :]-coords[i-10, :])\n",
    "        total_dist+=temp\n",
    "    speed=total_dist/duration\n",
    "    return speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@timeit\n",
    "def get_in_air_time(data):\n",
    "    data=data['Pressure'].as_matrix()\n",
    "    return (data<600).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@timeit\n",
    "def get_on_surface_time(data):\n",
    "    data=data['Pressure'].as_matrix()\n",
    "    return (data>600).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_velocity(f):\n",
    "    '''\n",
    "    change in direction and its position\n",
    "    \n",
    "    '''\n",
    "    data_pat=f\n",
    "    Vel = []\n",
    "    horz_Vel = []\n",
    "    horz_vel_mag = []\n",
    "    vert_vel_mag = []\n",
    "    vert_Vel = []\n",
    "    magnitude = []\n",
    "    timestamp_diff =  []\n",
    "    \n",
    "    t = 0\n",
    "    for i in range(len(data_pat)-2):\n",
    "        if t+10 <= len(data_pat)-1:\n",
    "            Vel.append(((data_pat['X'].as_matrix()[t+10] - data_pat['X'].as_matrix()[t])/(data_pat['Timestamp'].as_matrix()[t+10]-data_pat['Timestamp'].as_matrix()[t]) , (data_pat['Y'].as_matrix()[t+10]-data_pat['Y'].as_matrix()[t])/(data_pat['Timestamp'].as_matrix()[t+10]-data_pat['Timestamp'].as_matrix()[t])))\n",
    "            horz_Vel.append((data_pat['X'].as_matrix()[t+10] - data_pat['X'].as_matrix()[t])/(data_pat['Timestamp'].as_matrix()[t+10]-data_pat['Timestamp'].as_matrix()[t]))\n",
    "            \n",
    "            vert_Vel.append((data_pat['Y'].as_matrix()[t+10] - data_pat['Y'].as_matrix()[t])/(data_pat['Timestamp'].as_matrix()[t+10]-data_pat['Timestamp'].as_matrix()[t]))\n",
    "            magnitude.append(sqrt(((data_pat['X'].as_matrix()[t+10]-data_pat['X'].as_matrix()[t])/(data_pat['Timestamp'].as_matrix()[t+10]-data_pat['Timestamp'].as_matrix()[t]))**2 + (((data_pat['Y'].as_matrix()[t+10]-data_pat['Y'].as_matrix()[t])/(data_pat['Timestamp'].as_matrix()[t+10]-data_pat['Timestamp'].as_matrix()[t]))**2)))\n",
    "            timestamp_diff.append(data_pat['Timestamp'].as_matrix()[t+10]-data_pat['Timestamp'].as_matrix()[t])\n",
    "            horz_vel_mag.append(abs(horz_Vel[len(horz_Vel)-1]))\n",
    "            vert_vel_mag.append(abs(vert_Vel[len(vert_Vel)-1]))\n",
    "            t = t+10\n",
    "        else:\n",
    "            break\n",
    "    magnitude_vel = np.mean(magnitude)  \n",
    "    magnitude_horz_vel = np.mean(horz_vel_mag)\n",
    "    magnitude_vert_vel = np.mean(vert_vel_mag)\n",
    "    return Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_acceleration(f):\n",
    "    '''\n",
    "    change in direction and its velocity\n",
    "   \n",
    "    '''\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(f)\n",
    "    accl = []\n",
    "    horz_Accl =  []\n",
    "    vert_Accl = []\n",
    "    magnitude = []\n",
    "    horz_acc_mag = []\n",
    "    vert_acc_mag = []\n",
    "    for i in range(len(Vel)-2):\n",
    "        accl.append(((Vel[i+1][0]-Vel[i][0])/timestamp_diff[i] , (Vel[i+1][1]-Vel[i][1])/timestamp_diff[i]))\n",
    "        horz_Accl.append((horz_Vel[i+1]-horz_Vel[i])/timestamp_diff[i])\n",
    "        vert_Accl.append((vert_Vel[i+1]-vert_Vel[i])/timestamp_diff[i])\n",
    "        horz_acc_mag.append(abs(horz_Accl[len(horz_Accl)-1]))\n",
    "        vert_acc_mag.append(abs(vert_Accl[len(vert_Accl)-1]))\n",
    "        magnitude.append(sqrt(((Vel[i+1][0]-Vel[i][0])/timestamp_diff[i])**2 + ((Vel[i+1][1]-Vel[i][1])/timestamp_diff[i])**2))\n",
    "    \n",
    "    magnitude_acc = np.mean(magnitude)  \n",
    "    magnitude_horz_acc = np.mean(horz_acc_mag)\n",
    "    magnitude_vert_acc = np.mean(vert_acc_mag)\n",
    "    return accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jerk(f):\n",
    "    accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc = find_acceleration(f)\n",
    "    jerk = []\n",
    "    hrz_jerk = []\n",
    "    vert_jerk = []\n",
    "    magnitude = []\n",
    "    horz_jerk_mag = []\n",
    "    vert_jerk_mag = []\n",
    "    \n",
    "    for i in range(len(accl)-2):\n",
    "        jerk.append(((accl[i+1][0]-accl[i][0])/timestamp_diff[i] , (accl[i+1][1]-accl[i][1])/timestamp_diff[i]))\n",
    "        hrz_jerk.append((horz_Accl[i+1]-horz_Accl[i])/timestamp_diff[i])\n",
    "        vert_jerk.append((vert_Accl[i+1]-vert_Accl[i])/timestamp_diff[i])\n",
    "        horz_jerk_mag.append(abs(hrz_jerk[len(hrz_jerk)-1]))\n",
    "        vert_jerk_mag.append(abs(vert_jerk[len(vert_jerk)-1]))\n",
    "        magnitude.append(sqrt(((accl[i+1][0]-accl[i][0])/timestamp_diff[i])**2 + ((accl[i+1][1]-accl[i][1])/timestamp_diff[i])**2))\n",
    "        \n",
    "    magnitude_jerk = np.mean(magnitude)  \n",
    "    magnitude_horz_jerk = np.mean(horz_jerk_mag)\n",
    "    magnitude_vert_jerk = np.mean(vert_jerk_mag)\n",
    "    return jerk,magnitude,hrz_jerk,vert_jerk,timestamp_diff,magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCV_per_halfcircle(f):\n",
    "    data_pat=f\n",
    "    Vel = []\n",
    "    ncv = []\n",
    "    temp_ncv = 0\n",
    "    basex = data_pat['X'].as_matrix()[0]\n",
    "    for i in range(len(data_pat)-2):\n",
    "        if data_pat['X'].as_matrix()[i] == basex:\n",
    "            ncv.append(temp_ncv)\n",
    "            temp_ncv = 0\n",
    "            continue\n",
    "            \n",
    "        Vel.append(((data_pat['X'].as_matrix()[i+1] - data_pat['X'].as_matrix()[i])/(data_pat['Timestamp'].as_matrix()[i+1]-data_pat['Timestamp'].as_matrix()[i]) , (data_pat['Y'].as_matrix()[i+1]-data_pat['Y'].as_matrix()[i])/(data_pat['Timestamp'].as_matrix()[i+1]-data_pat['Timestamp'].as_matrix()[i])))\n",
    "        if Vel[len(Vel)-1] != (0,0):\n",
    "            temp_ncv+=1\n",
    "    ncv.append(temp_ncv)\n",
    "    #ncv = list(filter((2).__ne__, ncv))\n",
    "    ncv_Val = np.sum(ncv)/np.count_nonzero(ncv)\n",
    "    return ncv,ncv_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCA_per_halfcircle(f):\n",
    "    data_pat=f\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(f)\n",
    "    accl = []\n",
    "    nca = []\n",
    "    temp_nca = 0\n",
    "    basex = data_pat['X'].as_matrix()[0]\n",
    "    for i in range(len(Vel)-2):\n",
    "        if data_pat['X'].as_matrix()[i] == basex:\n",
    "            nca.append(temp_nca)\n",
    "            #print ('tempNCa::',temp_nca)\n",
    "            temp_nca = 0\n",
    "            continue\n",
    "            \n",
    "        accl.append(((Vel[i+1][0]-Vel[i][0])/timestamp_diff[i] , (Vel[i+1][1]-Vel[i][1])/timestamp_diff[i]))\n",
    "        if accl[len(accl)-1] != (0,0):\n",
    "            temp_nca+=1\n",
    "    nca.append(temp_nca)\n",
    "    nca = list(filter((2).__ne__, nca))\n",
    "    nca_Val = np.sum(nca)/np.count_nonzero(nca)\n",
    "    return nca,nca_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@timeit\n",
    "def get_features(f, parkinson_target):\n",
    "    global header_row\n",
    "    df=pd.read_csv(f, sep=';', header=None, names=header_row)\n",
    "    \n",
    "    df_static=df[df[\"Test_ID\"]==0]    # static test\n",
    "    df_dynamic=df[df[\"Test_ID\"]==1]    # dynamic test\n",
    "    df_stcp=df[df[\"Test_ID\"]==2]    # STCP\n",
    "    #df_static_dynamic=pd.concat([df_static, df_dynamic])\n",
    "    \n",
    "    initial_timestamp=df['Timestamp'][0]\n",
    "    df['Timestamp']=df['Timestamp']- initial_timestamp # offset timestamps\n",
    "    \n",
    "    duration_static = df_static['Timestamp'].as_matrix()[-1] if df_static.shape[0] else 1\n",
    "    duration_dynamic = df_dynamic['Timestamp'].as_matrix()[-1] if df_dynamic.shape[0] else 1\n",
    "    duration_STCP = df_stcp['Timestamp'].as_matrix()[-1] if df_stcp.shape[0] else 1\n",
    "\n",
    "    \n",
    "    data_point=[]\n",
    "    data_point.append(get_no_strokes(df_static) if df_static.shape[0] else 0) # no. of strokes for static test\n",
    "    data_point.append(get_no_strokes(df_dynamic) if df_dynamic.shape[0] else 0) # no. of strokes for dynamic test\n",
    "    data_point.append(get_speed(df_static) if df_static.shape[0] else 0) # speed for static test\n",
    "    data_point.append(get_speed(df_dynamic) if df_dynamic.shape[0] else 0) # speed for dynamic test\n",
    "\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of velocity, horizontal velocity and vertical velocity for static test\n",
    "    data_point.extend([magnitude_vel, magnitude_horz_vel,magnitude_vert_vel])\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of velocity, horizontal velocity and vertical velocity for dynamic test\n",
    "    data_point.extend([magnitude_vel, magnitude_horz_vel,magnitude_vert_vel])\n",
    "    \n",
    "    accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc=find_acceleration(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0)# magnitudes of acceleration, horizontal acceleration and vertical acceleration for static test        \n",
    "    data_point.extend([magnitude_acc,magnitude_horz_acc,magnitude_vert_acc])\n",
    "    accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc=find_acceleration(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0)# magnitudes of acceleration, horizontal acceleration and vertical acceleration for dynamic test        \n",
    "    data_point.extend([magnitude_acc,magnitude_horz_acc,magnitude_vert_acc])\n",
    "    \n",
    "    jerk,magnitude,hrz_jerk,vert_jerk,timestamp_diff,magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk=find_jerk(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of jerk, horizontal jerk and vertical jerk for static test\n",
    "    data_point.extend([magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk])\n",
    "    jerk,magnitude,hrz_jerk,vert_jerk,timestamp_diff,magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk=find_jerk(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of jerk, horizontal jerk and vertical jerk for dynamic test\n",
    "    data_point.extend([magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk])\n",
    "    \n",
    "    ncv,ncv_Val=NCV_per_halfcircle(df_static) if df_static.shape[0] else (0,0) # NCV for static test\n",
    "    data_point.append(ncv_Val)\n",
    "    ncv,ncv_Val=NCV_per_halfcircle(df_dynamic) if df_dynamic.shape[0] else (0,0) # NCV for dynamic test\n",
    "    data_point.append(ncv_Val)\n",
    "    \n",
    "    nca,nca_Val=NCA_per_halfcircle(df_static) if df_static.shape[0] else (0,0) # NCA for static test\n",
    "    data_point.append(nca_Val)\n",
    "    nca,nca_Val=NCA_per_halfcircle(df_dynamic) if df_dynamic.shape[0] else (0,0) # NCA for dynamic test\n",
    "    data_point.append(nca_Val)\n",
    "    \n",
    "    data_point.append(get_in_air_time(df_stcp) if df_stcp.shape[0] else 0) # in air time for STCP\n",
    "    data_point.append(get_on_surface_time(df_static) if df_static.shape[0] else 0) # on surface time for static test\n",
    "    data_point.append(get_on_surface_time(df_dynamic) if df_dynamic.shape[0] else 0) # on surface time for dynamic test\n",
    "    \n",
    "    data_point.append(parkinson_target)    # traget. 1 for parkinson. 0 for control.\n",
    "    \n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_feat=get_features(parkinson_file_list[35], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=[]\n",
    "for x in parkinson_file_list:\n",
    "    raw.append(get_features(x, 1))\n",
    "for x in control_file_list:\n",
    "    raw.append(get_features(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw=np.array(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_headers=['no_strokes_st', 'no_strokes_dy', 'speed_st', 'speed_dy', 'magnitude_vel_st' , 'magnitude_horz_vel_st' , 'magnitude_vert_vel_st', 'magnitude_vel_dy', 'magnitude_horz_vel_dy' , 'magnitude_vert_vel_dy', 'magnitude_acc_st' , 'magnitude_horz_acc_st' , 'magnitude_vert_acc_st','magnitude_acc_dy' , 'magnitude_horz_acc_dy' , 'magnitude_vert_acc_dy', 'magnitude_jerk_st', 'magnitude_horz_jerk_st' , 'magnitude_vert_jerk_st', 'magnitude_jerk_dy', 'magnitude_horz_jerk_dy' , 'magnitude_vert_jerk_dy', 'ncv_st', 'ncv_dy', 'nca_st', 'nca_dy', 'in_air_stcp','on_surface_st', 'on_surface_dy', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(raw, columns=features_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data shape', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=data[data['target']==1]\n",
    "neg=data[data['target']==0]\n",
    "\n",
    "train_pos=pos.head(pos.shape[0]-5)\n",
    "train_neg=neg.head(pos.shape[0]-5)\n",
    "train=pd.concat([train_pos, train_neg])\n",
    "print('train shape', train.shape)\n",
    "\n",
    "test_pos=pos.tail(5)\n",
    "test_neg=neg.tail(5)\n",
    "test=pd.concat([test_pos, test_neg])\n",
    "\n",
    "\n",
    "train_y=train['target']\n",
    "train_x=train.drop(['target'], axis=1)\n",
    "test_y=test['target']\n",
    "test_x=test.drop(['target'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction,actual):\n",
    "    correct = 0\n",
    "    not_correct = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == actual[i]:\n",
    "            correct+=1\n",
    "        else:\n",
    "            not_correct+=1\n",
    "    return (correct*100)/(correct+not_correct)\n",
    "\n",
    "\n",
    "def metrics(prediction,actual):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == actual[i] and actual[i]==1:\n",
    "            tp+=1\n",
    "        if prediction[i] == actual[i] and actual[i]==0:\n",
    "            tn+=1\n",
    "        if prediction[i] != actual[i] and actual[i]==0:\n",
    "            fp+=1\n",
    "        if prediction[i] != actual[i] and actual[i]==1:\n",
    "            fn+=1\n",
    "    metrics = {'Precision':(tp/(tp+fp+tn+fn)),'Recall':(tp/(tp+fn)),'F1':(2*(tp/(tp+fp+tn+fn))*(tp/(tp+fn)))/((tp/(tp+fp+tn+fn))+(tp/(tp+fn)))}\n",
    "    return (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "print('accuracy:',accuracy(test_y.tolist(), preds.tolist()), '%')\n",
    "print(metrics(test_y.tolist(), preds.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "print('accuracy:',accuracy(test_y.tolist(), preds.tolist()), '%')\n",
    "print(metrics(test_y.tolist(), preds.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=SVC()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "print('accuracy:',accuracy(test_y.tolist(), preds.tolist()), '%')\n",
    "print(metrics(test_y.tolist(), preds.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "print('accuracy:',accuracy(test_y.tolist(), preds.tolist()), '%')\n",
    "print(metrics(test_y.tolist(), preds.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=KNeighborsClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "print('accuracy:',accuracy(test_y.tolist(), preds.tolist()), '%')\n",
    "print(metrics(test_y.tolist(), preds.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(f, plot_func, t_id=0, x=None, y=None):\n",
    "    global header_row\n",
    "    df=pd.read_csv(f, sep=';', header=None, names=header_row)\n",
    "    df=df[df[\"Test_ID\"]==t_id]\n",
    "    initial_timestamp=df['Timestamp'][0]\n",
    "    df['Timestamp']=df['Timestamp']- initial_timestamp\n",
    "    plot_func(data=df, x=x, y=y, fit_reg=False, scatter_kws={\"s\": 0.5})\n",
    "    print(metrics(test_y.tolist(), preds.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pressure (Parkinsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(f=parkinson_file_list[35],  plot_func=sns.regplot, t_id=0, x='Timestamp', y='Pressure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pressure (Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(control_file_list[1], plot_func=sns.regplot, t_id=0, x='Timestamp', y='Pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(f=parkinson_file_list[35],  plot_func=sns.barplot, t_id=0, x='Timestamp', y='Pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(control_file_list[1], plot_func=sns.regplot, t_id=0, x='Timestamp', y='Pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
